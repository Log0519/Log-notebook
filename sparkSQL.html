<!DOCTYPE html>
<html>
<head>
<title>sparkSQL</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<style type="text/css">
/* GitHub stylesheet for MarkdownPad (http://markdownpad.com) */
/* Author: Nicolas Hery - http://nicolashery.com */
/* Version: b13fe65ca28d2e568c6ed5d7f06581183df8f2ff */
/* Source: https://github.com/nicolahery/markdownpad-github */

/* RESET
=============================================================================*/

html, body, div, span, applet, object, iframe, h1, h2, h3, h4, h5, h6, p, blockquote, pre, a, abbr, acronym, address, big, cite, code, del, dfn, em, img, ins, kbd, q, s, samp, small, strike, strong, sub, sup, tt, var, b, u, i, center, dl, dt, dd, ol, ul, li, fieldset, form, label, legend, table, caption, tbody, tfoot, thead, tr, th, td, article, aside, canvas, details, embed, figure, figcaption, footer, header, hgroup, menu, nav, output, ruby, section, summary, time, mark, audio, video {
  margin: 0;
  padding: 0;
  border: 0;
}

/* BODY
=============================================================================*/

body {
  font-family: Helvetica, arial, freesans, clean, sans-serif;
  font-size: 14px;
  line-height: 1.6;
  color: #333;
  background-color: #fff;
  padding: 20px;
  max-width: 960px;
  margin: 0 auto;
}

body>*:first-child {
  margin-top: 0 !important;
}

body>*:last-child {
  margin-bottom: 0 !important;
}

/* BLOCKS
=============================================================================*/

p, blockquote, ul, ol, dl, table, pre {
  margin: 15px 0;
}

/* HEADERS
=============================================================================*/

h1, h2, h3, h4, h5, h6 {
  margin: 20px 0 10px;
  padding: 0;
  font-weight: bold;
  -webkit-font-smoothing: antialiased;
}

h1 tt, h1 code, h2 tt, h2 code, h3 tt, h3 code, h4 tt, h4 code, h5 tt, h5 code, h6 tt, h6 code {
  font-size: inherit;
}

h1 {
  font-size: 28px;
  color: #000;
}

h2 {
  font-size: 24px;
  border-bottom: 1px solid #ccc;
  color: #000;
}

h3 {
  font-size: 18px;
}

h4 {
  font-size: 16px;
}

h5 {
  font-size: 14px;
}

h6 {
  color: #777;
  font-size: 14px;
}

body>h2:first-child, body>h1:first-child, body>h1:first-child+h2, body>h3:first-child, body>h4:first-child, body>h5:first-child, body>h6:first-child {
  margin-top: 0;
  padding-top: 0;
}

a:first-child h1, a:first-child h2, a:first-child h3, a:first-child h4, a:first-child h5, a:first-child h6 {
  margin-top: 0;
  padding-top: 0;
}

h1+p, h2+p, h3+p, h4+p, h5+p, h6+p {
  margin-top: 10px;
}

/* LINKS
=============================================================================*/

a {
  color: #4183C4;
  text-decoration: none;
}

a:hover {
  text-decoration: underline;
}

/* LISTS
=============================================================================*/

ul, ol {
  padding-left: 30px;
}

ul li > :first-child, 
ol li > :first-child, 
ul li ul:first-of-type, 
ol li ol:first-of-type, 
ul li ol:first-of-type, 
ol li ul:first-of-type {
  margin-top: 0px;
}

ul ul, ul ol, ol ol, ol ul {
  margin-bottom: 0;
}

dl {
  padding: 0;
}

dl dt {
  font-size: 14px;
  font-weight: bold;
  font-style: italic;
  padding: 0;
  margin: 15px 0 5px;
}

dl dt:first-child {
  padding: 0;
}

dl dt>:first-child {
  margin-top: 0px;
}

dl dt>:last-child {
  margin-bottom: 0px;
}

dl dd {
  margin: 0 0 15px;
  padding: 0 15px;
}

dl dd>:first-child {
  margin-top: 0px;
}

dl dd>:last-child {
  margin-bottom: 0px;
}

/* CODE
=============================================================================*/

pre, code, tt {
  font-size: 12px;
  font-family: Consolas, "Liberation Mono", Courier, monospace;
}

code, tt {
  margin: 0 0px;
  padding: 0px 0px;
  white-space: nowrap;
  border: 1px solid #eaeaea;
  background-color: #f8f8f8;
  border-radius: 3px;
}

pre>code {
  margin: 0;
  padding: 0;
  white-space: pre;
  border: none;
  background: transparent;
}

pre {
  background-color: #f8f8f8;
  border: 1px solid #ccc;
  font-size: 13px;
  line-height: 19px;
  overflow: auto;
  padding: 6px 10px;
  border-radius: 3px;
}

pre code, pre tt {
  background-color: transparent;
  border: none;
}

kbd {
    -moz-border-bottom-colors: none;
    -moz-border-left-colors: none;
    -moz-border-right-colors: none;
    -moz-border-top-colors: none;
    background-color: #DDDDDD;
    background-image: linear-gradient(#F1F1F1, #DDDDDD);
    background-repeat: repeat-x;
    border-color: #DDDDDD #CCCCCC #CCCCCC #DDDDDD;
    border-image: none;
    border-radius: 2px 2px 2px 2px;
    border-style: solid;
    border-width: 1px;
    font-family: "Helvetica Neue",Helvetica,Arial,sans-serif;
    line-height: 10px;
    padding: 1px 4px;
}

/* QUOTES
=============================================================================*/

blockquote {
  border-left: 4px solid #DDD;
  padding: 0 15px;
  color: #777;
}

blockquote>:first-child {
  margin-top: 0px;
}

blockquote>:last-child {
  margin-bottom: 0px;
}

/* HORIZONTAL RULES
=============================================================================*/

hr {
  clear: both;
  margin: 15px 0;
  height: 0px;
  overflow: hidden;
  border: none;
  background: transparent;
  border-bottom: 4px solid #ddd;
  padding: 0;
}

/* TABLES
=============================================================================*/

table th {
  font-weight: bold;
}

table th, table td {
  border: 1px solid #ccc;
  padding: 6px 13px;
}

table tr {
  border-top: 1px solid #ccc;
  background-color: #fff;
}

table tr:nth-child(2n) {
  background-color: #f8f8f8;
}

/* IMAGES
=============================================================================*/

img {
  max-width: 100%
}
</style>
</head>
<body>
<h1>一、创建 DataFrame</h1>
<p>在 Spark SQL 中 SparkSession 是创建 DataFrame 和执行 SQL 的入口，创建 DataFrame</p>
<h2>1. 三种方式创建：</h2>
<h3>① 通过 Spark 的数据源进行创建；</h3>
<pre><code>val df = spark.read.json(&quot;data/user.json&quot;)
</code></pre>

<h3>② 从一个存在的 RDD 进行转换；需要写清RDD里面的结构，即每个数据代表什么意思</h3>
<pre><code>val df = sc.makeRDD(List((&quot;zhangsan&quot;,30), (&quot;lisi&quot;,40))).map(t=&gt;User(t._1, t._2)).toDF
</code></pre>

<h3>③ 还可以从 Hive Table 进行查询返回</h3>
<h2>2. DataFrame操作</h2>
<h3>① 读取 JSON 文件创建 DataFrame</h3>
<pre><code>scala&gt; val df = spark.read.json(&quot;data/user.json&quot;)
df: org.apache.spark.sql.DataFrame = [age: bigint， username: string]
</code></pre>

<h3>② 对 DataFrame 创建一个临时表</h3>
<p><code>scala&gt; df.createOrReplaceTempView(&quot;people&quot;)</code></p>
<h3>③ 通过 SQL 语句实现查询全表</h3>
<pre><code>scala&gt; val sqlDF = spark.sql(&quot;SELECT * FROM people&quot;)
sqlDF: org.apache.spark.sql.DataFrame = [age: bigint， name: string]
</code></pre>

<h1>二、DataSet</h1>
<h3>1. 利用样例类创建</h3>
<pre><code>scala&gt; case class Person(name: String, age: Long)
defined class Person
scala&gt; val caseClassDS = Seq(Person(&quot;zhangsan&quot;,2)).toDS()
caseClassDS: org.apache.spark.sql.Dataset[Person] = [name: string, age: Long]
scala&gt; caseClassDS.show
</code></pre>

<h3>2. 使用基本类型的序列创建 DataSet</h3>
<pre><code>scala&gt; val ds = Seq(1,2,3,4,5).toDS
ds: org.apache.spark.sql.Dataset[Int] = [value: int]
scala&gt; ds.show
</code></pre>

<p><font color=#FF0000>注意：在实际使用的时候，很少用到把序列转换成DataSet，更多的是通过RDD来得到DataSet</font></p>
<p><font color=#FF0000>DataSet可以用 表名.属性 使用列</font></p>
<h1>三、RDD 转换为 DataSet</h1>
<h3>1. SparkSQL 能够自动将包含有 case 类的 RDD 转换成 DataSet，case 类定义了 table 的结构，case 类属性通过反射变成了表的列名。Case 类可以包含诸如 Seq 或者 Array 等复杂的结构。</h3>
<pre><code>scala&gt; case class User(name:String, age:Int)
defined class User
scala&gt; sc.makeRDD(List((&quot;zhangsan&quot;,30), (&quot;lisi&quot;,49))).map(t=&gt;User(t._1, t._2)).toDS
res11: org.apache.spark.sql.Dataset[User] = [name: string, age: int]
</code></pre>

<h3>2. DataSet 转换为 RDD</h3>
<p>DataSet 其实也是对 RDD 的封装，所以可以直接获取内部的 RDD</p>
<pre><code>scala&gt; case class User(name:String, age:Int)
defined class User
scala&gt; sc.makeRDD(List((&quot;zhangsan&quot;,30), (&quot;lisi&quot;,49))).map(t=&gt;User(t._1, 
t._2)).toDS
res11: org.apache.spark.sql.Dataset[User] = [name: string, age: int]
scala&gt; val rdd = res11.rdd
rdd: org.apache.spark.rdd.RDD[User] = MapPartitionsRDD[51] at rdd at 
&lt;console&gt;:25
scala&gt; rdd.collect
res12: Array[User] = Array(User(zhangsan,30), User(lisi,49))
</code></pre>

<h1>四、DataFrame 和 DataSet 转换</h1>
<h3>DataFrame 其实是 DataSet 的特例，所以它们之间是可以互相转换的。</h3>
<h3><font color=#FF0000>数据包含关系：DataSet &gt; DataFrame &gt; RDD ,从高转换到低很方便，低转换到高要给出高所需要的条件，DataFrame需要数据代表的意思，DataSet需要数据类型,DataFrame,DataSet都可以创建临时表</font></h3>
<p>RDD-----<strong>toDF</strong>-----&gt;DataFrame</p>
<p>DataFrame-----<strong>rdd</strong>-----&gt;RDD</p>
<p>DataFrame-----<strong>as[样例类]</strong>-----&gt;DataSet</p>
<p>DataSet-----<strong>toDF</strong>-----&gt;DataFrame</p>
<p>样例类RDD-----<strong>toDS</strong>-----&gt;DataSet，前提是rdd要把数据封装为样例类，直接使用也可以，但是不方便</p>
<p>DataSet-----<strong>rdd</strong>-----&gt;RDD</p>
<pre><code>scala&gt; case class User(name:String, age:Int)
defined class User
scala&gt; val df = sc.makeRDD(List((&quot;zhangsan&quot;,30), 
(&quot;lisi&quot;,49))).toDF(&quot;name&quot;,&quot;age&quot;)
df: org.apache.spark.sql.DataFrame = [name: string, age: int]
scala&gt; val ds = df.as[User]
ds: org.apache.spark.sql.Dataset[User] = [name: string, age: int]
</code></pre>

<h3>DataSet 转换为 DataFrame</h3>
<pre><code>scala&gt; val ds = df.as[User]
ds: org.apache.spark.sql.Dataset[User] = [name: string, age: int]
scala&gt; val df = ds.toDF
df: org.apache.spark.sql.DataFrame = [name: string, age: int]
</code></pre>

<h1>五. RDD、DataFrame、DataSet 三者的关系</h1>
<p>在 SparkSQL 中 Spark 为我们提供了两个新的抽象，分别是 <strong>DataFrame</strong> 和 <strong>DataSet</strong>。</p>
<p>他们和 RDD 有什么区别呢？首先从版本的产生上来看：</p>
<pre><code>Spark1.0 =&gt; RDD 
Spark1.3 =&gt; DataFrame
Spark1.6 =&gt; Dataset
</code></pre>

<p>如果同样的数据都给到这三个数据结构，他们分别计算之后，都会给出相同的结果。不
同是的他们的执行效率和执行方式。在后期的 Spark 版本中，DataSet 有可能会逐步取代 RDD
和 DataFrame 成为唯一的 API 接口。</p>
<h2>1. 三者的共性</h2>
<p>➢ RDD、DataFrame、DataSet 全都是 spark 平台下的分布式弹性数据集，为处理超大型数
据提供便利;</p>
<p>➢ 三者都有惰性机制，在进行创建、转换，如 map 方法时，不会立即执行，只有在遇到
Action 如 foreach 时，三者才会开始遍历运算; </p>
<p>➢ 三者有许多共同的函数，如 filter，排序等; </p>
<p>➢ 在对 DataFrame 和 Dataset 进行操作许多操作都需要这个包:import spark.implicits._（在
创建好 SparkSession 对象后尽量直接导入）</p>
<p>➢ 三者都会根据 Spark 的内存情况自动缓存运算，这样即使数据量很大，也不用担心会
内存溢出</p>
<p>➢ 三者都有 partition 的概念</p>
<p>➢ DataFrame 和 DataSet 均可使用模式匹配获取各个字段的值和类型</p>
<h2>2. 三者的区别</h2>
<h3>① RDD</h3>
<p>➢ RDD 一般和 spark mllib 同时使用</p>
<p>➢ RDD 不支持 sparksql 操作</p>
<h3>② DataFrame</h3>
<p>➢ 与 RDD 和 Dataset 不同，DataFrame 每一行的类型固定为 Row，每一列的值没法直
接访问，只有通过解析才能获取各个字段的值</p>
<p>➢ DataFrame 与 DataSet 一般不与 spark mllib 同时使用</p>
<p>➢ DataFrame 与 DataSet 均支持 SparkSQL 的操作，比如 select，groupby 之类，还能
注册临时表/视窗，进行 sql 语句操作</p>
<p>➢ DataFrame 与 DataSet 支持一些特别方便的保存方式，比如保存成 csv，可以带上表
头，这样每一列的字段名一目了然(后面专门讲解)</p>
<h3>③ DataSet</h3>
<p>➢ Dataset 和 DataFrame 拥有完全相同的成员函数，区别只是每一行的数据类型不同。</p>
<p>DataFrame 其实就是 DataSet 的一个特例 type DataFrame = Dataset[Row]</p>
<p>➢ DataFrame 也可以叫 Dataset[Row],每一行的类型是 Row，不解析，每一行究竟有哪
些字段，各个字段又是什么类型都无从得知，只能用上面提到的 getAS 方法或者共
性中的第七条提到的模式匹配拿出特定字段。而 Dataset 中，每一行是什么类型是
不一定的，在自定义了 case class 之后可以很自由的获得每一行的信息</p>
<h1>六. 数据访问方式</h1>
<h2>1. DataSet.show , DataFrame.show</h2>
<h2>2. 创建临时表后，spark . sql ( &quot; select * (Dataset可以用 ‘.属性’访问)  from  表名 &quot; )</h2>

</body>
</html>
<!-- This document was created with MarkdownPad, the Markdown editor for Windows (http://markdownpad.com) -->
